
---
title: "Clustering Credit Card Customers for Targeted Marketing using Unsupervised Learning"
output: html_document
---

# CSML1000 Project #2, by Group 8
Tamer Hanna <tamerh@my.yorku.ca>
Pete Gray <ptgray@my.yorku.ca>
Xiaohai Lu <yu271637@my.yorku.ca>
Haofeng Zhou <zhf85@my.yorku.ca>

```{r message=FALSE, warning=FALSE}
library(dplyr);
library(ggplot2);
library(knitr);
library(validate);
library(tidyverse);  # data manipulation
library(cluster);    # clustering algorithms



```

# OVERVIEW

Using data on the behaviour of credit card customers, we can use unsupervised learning to discover market segments that would be useful for targeting marketing strategies.

Our dataset has 19 columns of data on 9000 customers. Using K-means and PCA, we can determine an optimal number of market segments, discover the identifying properties of those segments, and be able to describe to the marketing department what the most significant behaviours of the people in those segments are. Marketing campaigns, then, can be targeted at, for example, impulse shoppers, big spenders, or people who have a hard time paying off their debts.

# BUSINESS UNDERSTANDING

Applying specific marketing strategies to different types of customers can improve results and reduce costs. Credit card customers can be profiled by the way they use, and pay off, their card.

## Business Objectives

Behavioural data can be mined to discover identifying features of clusters of customers who use their card in similar ways. The marketing department can use these insights to select target groups and optimize the marketing used to target them.
 
## Data Mining Goals

Using shallow algorithm unsupervised learning, we can discover clusters containing customers who exhibit similar behaviours. We can examine those 


# DATA UNDERSTANDING

## Collect Initial Data

Load the data from the local filesystem:

```{r message=FALSE, warning=FALSE}
#load data
df <- read.csv("credit-card-cust-behav-data.csv", stringsAsFactors = FALSE, header = TRUE, encoding  = "UTF-8")
```

Output a quick and dirty summary of the dataset, to ensure that we haven't loaded something scrambled, or the wrong thing:

```{r message=FALSE, warning=FALSE}
str(df)
```

## Describe Data

TO BE DONE.

## Explore Data

We'll plot some basic graphs, to ensure that the data conform to our limited domain understanding.

Balance vs. Credit Limit - we would expect to find a strong correlation here, even if only because those with small credit limits must have small balances. We certainly find it.

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(y=BALANCE, x=CREDIT_LIMIT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Balance vs. Credit Limit", x="Credit Limit", y="Balance") 
```

Maybe those who pay higher portion of balance purchase more one-offs? No, this graph is pretty junky and doesn't tell us anything.

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(y=ONEOFF_PURCHASES_FREQUENCY, x=PRC_FULL_PAYMENT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="One-off Purchase Frequency vs. Percent of full payment paid by user", x="Percent of full payment paid by user", y="One-off Purchase Frequency") 
```

Here we can see that there appears to be a correlation between a customers credit limit, and the frequency of their purchases. Not surprising, given that frequent purchasing is one factor that leads to an increased credit limit.

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(y=PURCHASES_FREQUENCY, x=CREDIT_LIMIT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Purchase Frequency vs. Credit Limit", x="Credit Limit", y="Purchase Frequency") 
```

Higher credit limit, more purchases? Maybe. This graph is an excellent showcase of "outliers". Only a small handful of the 9,000 records show a credit limit, or purchases, greater than $20,000. This will be one of our guiding intuitions when we get to the Data Cleaning phase.

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(y=PURCHASES, x=CREDIT_LIMIT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Purchases vs. Credit Limit", x="Credit Limit", y="Purchases") 
```

It would appear that those who pay off thier balance, do not take cash advances. No surprises, but a little hard to read because of the scale. One thing that appears in this graphs is a large "column" of people who are completely unlikely to pay off their entire balance, and take cash advances with very high frequency. (It is likely that the marketing department may have less interest in these people, than in others. But we'll see!)

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(y=CASH_ADVANCE_FREQUENCY, x=PRC_FULL_PAYMENT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Cash Advance Frequency vs. Percent of full payment paid by user", x="Percent of full payment paid by user", y="Cash Advance Frequency") 
```

One last graph, as part of our data exploration and sanity check. This one shows that there is a clear corrleation between a customer's credit limit, and their frequency of one-off purchases. By now, we can be comfortable that our data isn't erratic, and that our limited domain knowledge isn't out to lunch.

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(y=ONEOFF_PURCHASES_FREQUENCY, x=CREDIT_LIMIT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="One-off Purchase Frequency vs. Credit Limit", x="Credit Limit", y="One-off Purchase Frequency") 
```


## Verify Data Quality

Check if data greater than zero, look for missing data

```{r message=FALSE, warning=FALSE}
cf <- check_that(df, BALANCE > 0, BALANCE_FREQUENCY > 0, PURCHASES > 0, ONEOFF_PURCHASES > 0, INSTALLMENTS_PURCHASES > 0, CASH_ADVANCE > 0, PURCHASES_FREQUENCY > 0, ONEOFF_PURCHASES_FREQUENCY > 0, PURCHASES_INSTALLMENTS_FREQUENCY > 0, CASH_ADVANCE_FREQUENCY > 0, PURCHASES_TRX > 0, CREDIT_LIMIT > 0, MINIMUM_PAYMENTS > 0, PRC_FULL_PAYMENT > 0, TENURE > 0, BALANCE/CREDIT_LIMIT < 1)
summary(cf)
```

There appear to be a great number of zeroes. Let's look at that a little more closely:

```{r message=FALSE, warning=FALSE}
barplot(cf,main="Checks on the data set")
```

Lots of zero! Though, using our limitied knowledge of this domain, we can understand that this could be perfectly reasonable. In the first few lines of the above chart, we can see that a lot of people never make the full payment, a lot of people never get a cash advance, and some people didn't even make a purchase. Seems entirely reasonable. We can note too that the "zeroes" in PURCHASES_TRX, PURCHASES, and PURCHASES_FREQUENCY appear to be identical. In line with what we'd expect! 

Now let's include zero, and see how it all stacks up for Greater OR Equal to zero:

```{r message=FALSE, warning=FALSE}
cf <- check_that(df, BALANCE >= 0, BALANCE_FREQUENCY >= 0, PURCHASES >= 0, ONEOFF_PURCHASES >= 0, INSTALLMENTS_PURCHASES >= 0, CASH_ADVANCE >= 0, PURCHASES_FREQUENCY >= 0, ONEOFF_PURCHASES_FREQUENCY >= 0, PURCHASES_INSTALLMENTS_FREQUENCY >= 0, CASH_ADVANCE_FREQUENCY >= 0, PURCHASES_TRX >= 0, CREDIT_LIMIT >= 0, MINIMUM_PAYMENTS >= 0, PRC_FULL_PAYMENT >= 0, TENURE >= 0, BALANCE/CREDIT_LIMIT <= 1)
summary(cf)
```

Our only serious problems remaining are some N/As in Minimum Payment. We also see that a few people have a balance greater than their credit limit - this would not constitue a problem with the data. Simply a problem the customer in question is having. (Their balance is higher than their credit limit)

```{r message=FALSE, warning=FALSE}
barplot(cf,main="Checks on the data set")
```

As a last look at the data, let's look at the variance of the individual columns.

```{r message=FALSE, warning=FALSE}
var(df$BALANCE) 
var(df$BALANCE_FREQUENCY)               
var(df$PURCHASES)                       
var(df$ONEOFF_PURCHASES)                
var(df$INSTALLMENTS_PURCHASES)          
var(df$CASH_ADVANCE)                    
var(df$PURCHASES_FREQUENCY)             
var(df$ONEOFF_PURCHASES_FREQUENCY)      
var(df$PURCHASES_INSTALLMENTS_FREQUENCY)
var(df$CASH_ADVANCE_FREQUENCY)          
var(df$CASH_ADVANCE_TRX)                
var(df$PURCHASES_TRX)                   
var(df$CREDIT_LIMIT)                    
var(df$PAYMENTS)                        
var(df$MINIMUM_PAYMENTS)                
var(df$PRC_FULL_PAYMENT)                
var(df$TENURE)
```

The data appear to have more information in the "amount" columns, and less in the "frequency" columns. We'll test this assumption when we try some modelling.


# DATA PREPARATION

## Select Data

We saw that MINIMUM_PAYMENTS is riddled with N/As, so let's remove that column. This should be compatible with our Business Objectives, as we're hoping to get people to spend, rather than optimize their repayments.

```{r message=FALSE, warning=FALSE}
df$MINIMUM_PAYMENTS <- NULL
```

There was one nasty little N/A in CREDIT_LIMIT, let's remove that row so we can use that column.

```{r message=FALSE, warning=FALSE}
df <- na.omit(df)
```

## Clean Data

During the Data Exploration phase, we discovered that of the 9,000 rows of data, there are a handful that have much higher values that all the rest. While we are certainly interested in these customers, they can be easily found without expensive machine learning. Simple code can be used to scrape off the customers with a credit limit, or purchases, higher than some amount.

As we're hoping to come up with customer segments that are a significant portion of our customer base, perhaps containing hundreds or thousands of customers, let's look at "capping" some of these extreme values, to see if it moderates these few extreme outliers and suggests more patterns at a smaller scale.

First, here's the graph that drew this to our attention:

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(y=PURCHASES, x=CREDIT_LIMIT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Purchases vs. Credit Limit", x="Credit Limit", y="Purchases") 
```

Let's cap those two variables at $20,000 and see if it seems a little less crazy:

```{r message=FALSE, warning=FALSE}
df$CREDIT_LIMIT[df$CREDIT_LIMIT > 20000 ] <- 20000
df$PURCHASES[df$PURCHASES > 20000 ] <- 20000
df %>% ggplot(aes(y=PURCHASES, x=CREDIT_LIMIT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Purchases vs. Credit Limit", x="Credit Limit", y="Purchases") 
```

How about if we capped them at a smaller number, would it get silly? Or would it show us more variance inside the dense part of the distribution?

```{r message=FALSE, warning=FALSE}
df$CREDIT_LIMIT[df$CREDIT_LIMIT > 10000 ] <- 10000
df$PURCHASES[df$PURCHASES > 10000 ] <- 10000
df %>% ggplot(aes(y=PURCHASES, x=CREDIT_LIMIT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Purchases vs. Credit Limit", x="Credit Limit", y="Purchases") 
```

Well, I don't like that. Too many Credit Limits at or above 10,000. Though, that doesn't seem a problem with Purchases.

Let's reset the data, and cap the values differently.

```{r message=FALSE, warning=FALSE}
#load data
df <- read.csv("credit-card-cust-behav-data.csv", stringsAsFactors = FALSE, header = TRUE, encoding  = "UTF-8")
df$MINIMUM_PAYMENTS <- NULL
df <- na.omit(df)
 
df$CREDIT_LIMIT[df$CREDIT_LIMIT > 20000 ] <- 20000
df$PURCHASES[df$PURCHASES > 10000 ] <- 10000
 
df %>% ggplot(aes(y=PURCHASES, x=CREDIT_LIMIT)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Purchases vs. Credit Limit", x="Credit Limit", y="Purchases") 
```

That seems much better, both for the purchases, and the credit limit.

Let's look at one variable at a time, to ensure we haven't created a big distortion out at the high end:

```{r message=FALSE, warning=FALSE}
hist(df$CREDIT_LIMIT)
```

```{r message=FALSE, warning=FALSE}
hist(df$PURCHASES)
```

Seems okay. Let's look at the distribution of some of the other big-value columns that we're most interested in, to see how they look.

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(y=PAYMENTS, x=CASH_ADVANCE)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Payments vs. Cash Advances", x="Cash Advances", y="Payments") 
```

Wow, that's some serious outliers. Let's cap them.

```{r message=FALSE, warning=FALSE}

df$PAYMENTS[df$PAYMENTS > 12000 ] <- 12000
df$CASH_ADVANCE[df$CASH_ADVANCE > 25000 ] <- 25000
 
df %>% ggplot(aes(y=PAYMENTS, x=CASH_ADVANCE)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="Payments vs. Cash Advances", x="Cash Advances", y="Payments") 
```

Two more columns we're very interested in using need to be check for this:

```{r message=FALSE, warning=FALSE}
df %>% ggplot(aes(y=ONEOFF_PURCHASES, x=INSTALLMENTS_PURCHASES)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="ONEOFF_PURCHASES vs. INSTALLMENTS_PURCHASES", x="INSTALLMENTS_PURCHASES", y="ONEOFF_PURCHASES") 
```

A very few extreme outliers. Let's bring them a little closer to the group:

```{r message=FALSE, warning=FALSE}

df$ONEOFF_PURCHASES[df$ONEOFF_PURCHASES > 12000 ] <- 12000
df$INSTALLMENTS_PURCHASES[df$INSTALLMENTS_PURCHASES > 8000 ] <- 8000

df %>% ggplot(aes(y=ONEOFF_PURCHASES, x=INSTALLMENTS_PURCHASES)) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) + 
  labs(title="ONEOFF_PURCHASES vs. INSTALLMENTS_PURCHASES", x="INSTALLMENTS_PURCHASES", y="ONEOFF_PURCHASES") 
```

Let us try with this, and we can compare it to fresh, uncapped data when we get to modelling.

## Construct Data



## Integrate Data
## Format Data


# MODELING

## Select Modeling Technique

K-Means, we try first.

## Generate Test Design

# Determine number of clusters

Run tests on various numbers of clusters to look for an "elbow" which will suggest how many useful clusters we can hope to get from this data.

First, we'll do it with many columns, so we can compare to much smaller number more of our choosing.

```{r message=FALSE, warning=FALSE}

cld <- df[c(2:14)]

wss <- (nrow(cld)-1)*sum(apply(cld,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(cld,
   centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares") 
 
```

Here's the same test, but performed only with BALANCE, PURCHASES, ONEOFF_PURCHASES, INSTALLMENTS_PURCHASES, and CASH_ADVANCE.

```{r message=FALSE, warning=FALSE}

cld <- df[c(2,4:7)]

wss <- (nrow(cld)-1)*sum(apply(cld,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(cld,
   centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares") 
 
```

We've definitely lost some of our predicting power with only those columns - let's add CREDIT_LIMIT back in to see if that helps:

```{r message=FALSE, warning=FALSE}

cld <- df[c(2,4:7, 14)]

wss <- (nrow(cld)-1)*sum(apply(cld,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(cld,
   centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares") 
 
```

This is surprisingly similar to the graph we obtained using 12 rows above.

In terms of how many clusters we should choose, these graphs could be suggesting anywhere from 4 to 7, depending on how we read them. We'll try a few things.

Here's one with 6 clusters:

```{r message=FALSE, warning=FALSE}
# K-Means Cluster Analysis
fit <- kmeans(cld, 6) # 6 cluster solution
# get cluster means
aggregate(cld,by=list(fit$cluster),FUN=mean)
# append cluster assignment
cld <- data.frame(cld, fit$cluster)
## Assess Model
```

Sure, but how does this look WITHOUT all that capping?

```{r message=FALSE, warning=FALSE}

df0 <- read.csv("credit-card-cust-behav-data.csv", stringsAsFactors = FALSE, header = TRUE, encoding  = "UTF-8")
df0 <- na.omit(df0)
cld2 <- df0[c(2,4:7,14)]

wss <- (nrow(cld2)-1)*sum(apply(cld2,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(cld2,
   centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares") 
 
```

And a clustering into 6, with the uncapped data, yeilds almost identical results.

```{r message=FALSE, warning=FALSE}
# K-Means Cluster Analysis
fit <- kmeans(cld2, 6) # 6 cluster solution
# get cluster means
aggregate(cld2,by=list(fit$cluster),FUN=mean)
# append cluster assignment
cld2 <- data.frame(cld2, fit$cluster)
## Assess Model
```

This 6-cluster solution clearly nets us 6 groups we can identify - Big Spenders, Cash Advancers, Smaller Cash Advancers, Balance Carriers, Balance Payers, and Light Users.

It is interesting that whether or not we cap, with 6 clusters, we get two groups of "cash advancers", which differ only in terms of credit limit.

Let's do one with 5 clusters:

```{r message=FALSE, warning=FALSE}
# K-Means Cluster Analysis
fit <- kmeans(cld, 5) # 5 cluster solution
# get cluster means
aggregate(cld,by=list(fit$cluster),FUN=mean)
# append cluster assignment
cld <- data.frame(cld, fit$cluster)
## Assess Model
```

This seems cleaner and easier to interpret. Big Spenders, Cash Advancers,

And lastly one with 4 clusters:

```{r message=FALSE, warning=FALSE}
# K-Means Cluster Analysis
fit <- kmeans(cld, 4) # 4 cluster solution
# get cluster means
aggregate(cld,by=list(fit$cluster),FUN=mean)
# append cluster assignment
cld <- data.frame(cld, fit$cluster)
## Assess Model
```

This only seems to give us Cash Advancers, and Small, Medium, and Large spenders. Very accurately predicted, I'm sure, but not as useful as the clusters we got with 5 and 6 clusters.

Let's try to export that 6 cluster version as an .rda file for use in the Shiny App:

```{r message=FALSE, warning=FALSE}
dfx <- read.csv("credit-card-cust-behav-data.csv", stringsAsFactors = FALSE, header = TRUE, encoding  = "UTF-8")
dfx <- na.omit(dfx)
clx <- dfx[c(2,4:7,14)]

wss <- (nrow(clx)-1)*sum(apply(clx,2,var))
for (i in 2:10) wss[i] <- sum(kmeans(clx,
   centers=i)$withinss)
plot(1:10, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares") 
# K-Means Cluster Analysis
fit <- kmeans(clx, 6) # 6 cluster solution
# get cluster means
aggregate(clx,by=list(fit$cluster),FUN=mean)
# append cluster assignment
clx <- data.frame(clx, fit$cluster)
## Assess Model
#save(clx , file = 'CreditCardBehaviour6Clusters.rda')
```







#first scaling the data
dft <- read.csv("credit-card-cust-behav-data.csv", stringsAsFactors = TRUE, header = TRUE, encoding  = "UTF-8")
dft <- dft[,-1]
dft <- na.omit(dft)
scaled.dft <- scale(dft)

# Credit Card user  matrix
scaled.dft <- dist(scaled.dft, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(scaled.dft, method = "average" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.9, hang = -2)




# EVALUATION

Evaluation? Coming soon.

## Evaluate Results
## Approved Models
## Determine Next Steps


# DEPLOYMENT

## Plan Deployment
## Plan Monitoring and Maintenance
## Produce Final Report -------------------------------  KNIT THIS .Rmd FILE WHEN DONE!!!!
## Review Project